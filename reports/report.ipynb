{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CALITP_BQ_MAX_BYTES\"] = str(800_000_000_000)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from io import StringIO\n",
    "from calitp.tables import tbl\n",
    "from calitp import query_sql\n",
    "from datetime import date, datetime\n",
    "from siuba import *\n",
    "from siuba.dply.forcats import fct_rev\n",
    "from plotnine import *\n",
    "\n",
    "def friendly_date(x): \n",
    "    return datetime.strptime(x, \"%Y-%m-%d\").strftime(\"%b %d\")\n",
    "\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters",
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "DIR_PATH=\"2022/05/4\"\n",
    "\n",
    "#CALITP_ITP_ID = 10\n",
    "#CALITP_ITP_ID=45\n",
    "#CALITP_ITP_ID=269\n",
    "#CALITP_ITP_ID=108\n",
    "#CALITP_ITP_ID=106\n",
    "CALITP_ITP_ID=350\n",
    "CALITP_URL_NUMBER = 0\n",
    "DEBUG = False\n",
    "\n",
    "DATE_START = \"2022-05-01\"\n",
    "DATE_END = \"2022-05-31\"\n",
    "PUBLISH_DATE = \"2022-06-01\"\n",
    "\n",
    "CHANGE_PLOT_BREAKS = [\"Added\", \"Removed\", \"Unchanged\"]\n",
    "CHANGE_PLOT_VALUES = [\"#51BF9D\",\"#E16B26\", \"#8CBCCB\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE_TODAY=date.today()\n",
    "START_MONTH_DAY = friendly_date(DATE_START)\n",
    "END_MONTH_DAY = friendly_date(DATE_END)\n",
    "\n",
    "WEEK_MARKERS = pd.date_range(DATE_START, DATE_END, freq=\"W\").astype(str).tolist()\n",
    "BIWEEKLY_DT = pd.date_range(DATE_START, DATE_END, freq=\"2W\")\n",
    "BIWEEKLY_MARKERS = BIWEEKLY_DT.astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "if not DEBUG:\n",
    "    warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# Convenience functions ----\n",
    "\n",
    "filter_start = filter(\n",
    "    _.calitp_extracted_at <= DATE_START,\n",
    "    _.calitp_deleted_at.fillna(\"2099-01-01\") > DATE_START,\n",
    ")\n",
    "\n",
    "filter_end = filter(\n",
    "    _.calitp_extracted_at <= DATE_END,\n",
    "    _.calitp_deleted_at.fillna(\"2099-01-01\") > DATE_END,\n",
    ")\n",
    "\n",
    "filter_itp = filter(\n",
    "    _.calitp_itp_id == CALITP_ITP_ID, _.calitp_url_number == CALITP_URL_NUMBER\n",
    ")\n",
    "\n",
    "def collect_to_dict(df):\n",
    "    \"\"\"Return the first row of a DataFrame as a dictionary.\n",
    "    \n",
    "    If there are no rows, then all dictionary values are None.\n",
    "    \"\"\"\n",
    "    \n",
    "    result = collect(df)\n",
    "    \n",
    "    if len(result) == 0:\n",
    "        return {k: None for k in result.columns}\n",
    "    # elif len(result) > 1:\n",
    "    #     raise ValueError(\"table contained more than 1 row\")\n",
    "    else:\n",
    "        return result.iloc[0,:].to_dict()\n",
    "\n",
    "\n",
    "select_rm_calitp = select(\n",
    "    -_.calitp_itp_id,\n",
    "    -_.calitp_url_number,\n",
    "    -_.calitp_hash,\n",
    "    -_.calitp_extracted_at,\n",
    "    -_.calitp_deleted_at,\n",
    ")\n",
    "\n",
    "def percent_format(labels):\n",
    "    return [\"{:.0f}%\".format(v*100) for v in labels]\n",
    "\n",
    "def query_id_changes(start_table, end_table, id_vars):\n",
    "    \"\"\"Calculate id variables that are removed, added, or unchanged between tables.\n",
    "    \n",
    "    It works by adding a special column to each table, performing a full join,\n",
    "    then checking where the special column is null.\n",
    "    \"\"\"\n",
    "    sym_id_vars = [_[k] for k in id_vars]\n",
    "\n",
    "    is_in_start = start_table >> select(*id_vars) >> mutate(is_in_start=True)\n",
    "    is_in_end = end_table >> select(*id_vars) >> mutate(is_in_end=True)\n",
    "\n",
    "    baseline = start_table >> count(*id_vars) >> rename(n_baseline=\"n\")\n",
    "    tallies = (\n",
    "        is_in_start\n",
    "        >> full_join(_, is_in_end, id_vars)\n",
    "        >> count(*sym_id_vars, _.is_in_start, _.is_in_end)\n",
    "        >> mutate(\n",
    "            status=case_when(\n",
    "                _,\n",
    "                {\n",
    "                    _.is_in_end.isna(): \"Removed\",\n",
    "                    _.is_in_start.isna(): \"Added\",\n",
    "                    True: \"Unchanged\",\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "        >> count(*sym_id_vars[:-1], _.status)\n",
    "        >> group_by(*sym_id_vars[:-1])\n",
    "        >> mutate(percent=_.n / _.n.sum())\n",
    "    )\n",
    "\n",
    "    return tallies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# Data ====\n",
    "# 1. High level feed info ----\n",
    "\n",
    "## start/end dates now dt.date, need to format downstream?...\n",
    "## collect here for 1 less query, small table after all\n",
    "tbl_dim_feeds = (tbl.views.gtfs_schedule_dim_feeds()\n",
    "     >> filter_end\n",
    "     >> filter_itp\n",
    "     >> collect()\n",
    "    )\n",
    "\n",
    "status = (\n",
    "    tbl_dim_feeds   \n",
    "    >> select(_.calitp_agency_name, _.raw_gtfs_schedule_url)\n",
    "    >> pipe(collect_to_dict)\n",
    "    )\n",
    "\n",
    "feed_info = (tbl_dim_feeds\n",
    "    >> select(_.feed_publisher_name, _.feed_publisher_url, _.feed_lang,\n",
    "           _.default_lang, _.feed_start_date, _.feed_end_date, _.feed_version,\n",
    "           _.feed_contact_email, _.feed_contact_url)\n",
    "    >> pipe(collect_to_dict)\n",
    "    )\n",
    "\n",
    "if feed_info['feed_start_date']:\n",
    "    feed_info['feed_start_date'] = feed_info['feed_start_date'].strftime('%Y%m%d')\n",
    "if feed_info['feed_end_date']:\n",
    "    feed_info['feed_end_date'] = feed_info['feed_end_date'].strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "## new table!\n",
    "tbl_daily_service = query_sql(f\"\"\"\n",
    "SELECT *\n",
    "FROM `cal-itp-data-infra-staging.eric_staging.reports_daily_service_routes_stops`\n",
    "WHERE calitp_itp_id = {CALITP_ITP_ID} AND calitp_url_number = {CALITP_URL_NUMBER}\n",
    "AND service_date BETWEEN '{DATE_START}' AND '{DATE_END}'\n",
    "\"\"\")\n",
    "tbl_daily_service['ttl_service_hours'] = tbl_daily_service.ttl_service_hours.fillna(0)\n",
    "tbl_daily_service = tbl_daily_service >> arrange(_.service_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5. Route and Stop info\n",
    "## specific date makes more sense (end of month, maintain consistency)\n",
    "end_date_routes_stops = (tbl_daily_service\n",
    "                         >> filter(_.service_date == BIWEEKLY_DT[-1])\n",
    "                         >> select(_.n_routes, _.n_stops)\n",
    "                        )\n",
    "\n",
    "feed_info[\"n_routes\"] = int(end_date_routes_stops.iloc[0, 0])\n",
    "feed_info[\"n_stops\"] = int(end_date_routes_stops.iloc[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Monthly metrics ----\n",
    "# Service hours per day\n",
    "\n",
    "# number of days where a feed did not have any trips in service\n",
    "n_days_no_service = (\n",
    "    tbl_daily_service\n",
    "    >> filter(_.ttl_service_hours == 0.0)\n",
    "    >> pipe(lambda d: {\"n\": d.shape[0]})\n",
    ")\n",
    "\n",
    "# 3. Stop and Route ID Changes ----\n",
    "\n",
    "query_id_changes = (\n",
    "    filter(_.metric_period == \"month\")\n",
    "    >> filter_itp \n",
    "    # note that metric dates for calendar type roll-ups (e.g. month, quarter) is the first day of the next\n",
    "    # period (e.g. metric date June 1st rolls up May).\n",
    "    >> filter(_.metric_date == PUBLISH_DATE)\n",
    "    >> rename(status = \"change_status\")\n",
    "    >> mutate(percent = _.n / _.n.sum())\n",
    "    >> collect()\n",
    ")\n",
    "\n",
    "tbl_stops_changed = tbl.views.gtfs_schedule_fact_stop_id_changes() >> query_id_changes\n",
    "tbl_routes_changed = tbl.views.gtfs_schedule_fact_route_id_changes() >> query_id_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# 4. Feed files being checked for ----\n",
    "\n",
    "file_categories = pd.DataFrame(\n",
    "    {\n",
    "        \"shapes.txt\": \"Visual display\",\n",
    "        \"pathways.txt\": \"Navigation\",\n",
    "        \"levels.txt\": \"Navigation\",\n",
    "        \"fare_rules.txt\": \"Fares\",\n",
    "        \"fare_leg_rules.txt\": \"Fares\",\n",
    "        \"feed_info.txt\": \"Technical contacts\",\n",
    "    }.items(),\n",
    "    columns=[\"name\", \"category\"],\n",
    ")\n",
    "\n",
    "importance = [\"Visual display\", \"Navigation\", \"Fares\", \"Technical contacts\"]\n",
    "\n",
    "tbl_file_check = (\n",
    "    tbl.gtfs_schedule_history.calitp_files_updates()\n",
    "    >> filter_itp\n",
    "    >> filter(_.calitp_extracted_at.isin(BIWEEKLY_MARKERS))\n",
    "    >> select(_.name, _.calitp_extracted_at)\n",
    "    >> collect()\n",
    "    >> right_join(_, file_categories, [\"name\"])\n",
    "    >> mutate(\n",
    "        calitp_extracted_at=_.calitp_extracted_at.fillna(\"missing\").astype(str),\n",
    "        success=\"✅\",\n",
    "    )\n",
    "    >> spread(_.calitp_extracted_at, _.success)\n",
    "    >> select(-_.missing)\n",
    "    >> arrange(_.category.apply(importance.index))\n",
    "    >> select(_.category, _.contains(\"\"))\n",
    "    >> pipe(_.fillna(\"\"))\n",
    ")\n",
    "\n",
    "# Analyze validation notices ----\n",
    "\n",
    "from sqlalchemy.sql import func\n",
    "\n",
    "tbl_biweekly = (\n",
    "    tbl.views.dim_date()\n",
    "    >> filter(_.full_date.isin(BIWEEKLY_MARKERS))\n",
    "    >> select(_.date == _.full_date)\n",
    ")\n",
    "\n",
    "tbl_validation_notices_raw = (\n",
    "    tbl.gtfs_schedule_type2.validation_notices()\n",
    "    >> filter_itp\n",
    "    >> inner_join(\n",
    "        _,\n",
    "        tbl_biweekly,\n",
    "        sql_on = (lambda lhs, rhs:\n",
    "                  (lhs.calitp_extracted_at <= rhs.date) &\n",
    "                  (func.coalesce(lhs.calitp_deleted_at, \"2099-01-01\") > rhs.date)\n",
    "        )\n",
    "    )\n",
    "    # do a simple count of the number of notices on each date\n",
    "    >> select(_.date, _.code, _.severity)\n",
    "    >> count(_.date, _.code, _.severity)\n",
    "    >> collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame with correct columns and no rows\n",
    "_validation_empty = pd.DataFrame(\n",
    "    dict([(\"code\", []), (\"severity\", []), *zip(BIWEEKLY_MARKERS, [tuple()] * len(BIWEEKLY_MARKERS))]),\n",
    ")\n",
    "\n",
    "# spread validation notices so dates are columns, and counts are values\n",
    "# TODO: when we spread the dates out, the counts sometimes can become floats,\n",
    "# so have decimals, which might look funky. We can fix this by either converting\n",
    "# the counts to strings before the code below\n",
    "if not tbl_validation_notices_raw.shape[0]:\n",
    "    _tbl_validation_notices = _validation_empty\n",
    "else:\n",
    "    out_wide = (\n",
    "        tbl_validation_notices_raw >> mutate(date=_.date.astype(str)) >> spread(_.date, _.n)\n",
    "    )\n",
    "    \n",
    "    _tbl_validation_notices = pd.concat(\n",
    "        [_validation_empty, out_wide], ignore_index=True\n",
    "    ).fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame with correct columns and no rows\n",
    "_validation_severity_empty = pd.DataFrame(\n",
    "    dict([(\"code\", []), (\"severity\", []), *zip(BIWEEKLY_MARKERS, [tuple()] * len(BIWEEKLY_MARKERS))]),\n",
    ")\n",
    "\n",
    "# spread validation notices so dates are columns, and counts are values\n",
    "# TODO: when we spread the dates out, the counts sometimes can become floats,\n",
    "# so have decimals, which might look funky. We can fix this by either converting\n",
    "# the counts to strings before the code below\n",
    "if not tbl_validation_notices_raw.shape[0]:\n",
    "    _tbl_validation_severity = _validation_severity_empty\n",
    "else:\n",
    "    out_wide = (\n",
    "        tbl_validation_notices_raw >> mutate(date=_.date.astype(str)) >> spread(_.date, _.n)\n",
    "    )\n",
    "    \n",
    "    _tbl_validation_severity = pd.concat(\n",
    "        [_validation_severity_empty, out_wide], ignore_index=True\n",
    "    ).fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tbl_code_descriptions = (\n",
    "    tbl.views.validation_code_descriptions()\n",
    "    >> select(_.code, _.human_readable_description)\n",
    "    >> collect()\n",
    "    # convert code to snakecase,\n",
    "    # note that this currently screws up cases like IO -> i_o\n",
    "    >> mutate(code = _.code.str.replace(r'(?<!^)(?=[A-Z])', '_').str.lower().str.replace(\"_notice$\", \"\"))\n",
    ")\n",
    "\n",
    "tbl_validation_notices_unfiltered = (\n",
    "    _tbl_validation_notices \n",
    "    >> inner_join(_, tbl_code_descriptions, [\"code\"])\n",
    "    >> select(_.code, _.human_readable_description, _.contains(\"\"))\n",
    "    #>> rename(error_name = _.code, error_description=_.human_readable_description)\n",
    ")\n",
    "\n",
    "tbl_validation_severity = (\n",
    "    _tbl_validation_severity\n",
    "    >> inner_join(_, tbl_code_descriptions, [\"code\"])\n",
    "    >> select(_.code, _.human_readable_description, _.severity)\n",
    ")\n",
    "\n",
    "# hackily remove float decimal values by casting to string and removing \".0\"\n",
    "# see https://stackoverflow.com/a/68693516/269834\n",
    "for col in tbl_validation_notices_unfiltered.columns[2:]:\n",
    "    tbl_validation_notices_unfiltered[col] = tbl_validation_notices_unfiltered[col].astype(str).apply(lambda x: x.replace('.0',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter tbl_validation for severity\n",
    "\n",
    "tbl_validation_notices = (\n",
    "    tbl_validation_notices_unfiltered\n",
    "    >> select(-_.severity)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "source": [
    "## Dump data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# Note that everything is saved in this cell, except for plots,\n",
    "# which are saved in the cell they're defined in\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def to_rowspan_table(df, span_col):\n",
    "    d = df.to_dict(orient=\"split\")\n",
    "    row_span = df.groupby(span_col)[span_col].transform(\"count\")\n",
    "    not_first = df[span_col].duplicated()\n",
    "    \n",
    "    row_span[not_first] = 0\n",
    "    \n",
    "    d[\"rowspan\"] = row_span.tolist()\n",
    "    return d\n",
    "\n",
    "\n",
    "out_dir = Path(f\"outputs/{DIR_PATH}/data\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "json.dump(feed_info, open(out_dir / \"1_feed_info.json\", \"w\"))\n",
    "json.dump(status, open(out_dir / \"1_status.json\", \"w\"))\n",
    "\n",
    "tbl_daily_service_hours.to_json(\n",
    "    out_dir / \"2_daily_service_hours.json\", orient=\"records\"\n",
    ")\n",
    "json.dump(n_days_no_service, open(out_dir / \"2_n_days_no_service.json\", \"w\"))\n",
    "\n",
    "tbl_stops_changed.to_json(out_dir / \"3_stops_changed.json\", orient=\"records\")\n",
    "tbl_routes_changed.to_json(out_dir / \"3_routes_changed.json\", orient=\"records\")\n",
    "\n",
    "json.dump(to_rowspan_table(tbl_file_check, \"category\"), open(out_dir / \"4_file_check.json\", \"w\"))\n",
    "tbl_validation_severity.to_json(out_dir / \"4_validation_severity.json\", orient=\"split\")\n",
    "tbl_validation_notices.to_json(out_dir / \"5_validation_notices.json\", orient=\"split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edasmalchi/.venv/reports/lib/python3.9/site-packages/plotnine/ggplot.py:719: PlotnineWarning: Saving 4 x 4 in image.\n",
      "/Users/edasmalchi/.venv/reports/lib/python3.9/site-packages/plotnine/ggplot.py:722: PlotnineWarning: Filename: outputs/2022/05/4/data/2_service_hours.png\n"
     ]
    }
   ],
   "source": [
    "## Service Hours Plot\n",
    "\n",
    "p = (\n",
    "#     tbl_daily_service_hours\n",
    "    tbl_daily_service\n",
    "    >> ggplot(aes(\"service_date\", \"ttl_service_hours\", group = 1))\n",
    "    + geom_line()\n",
    "    + geom_point()\n",
    "    + theme_minimal(base_size=16)    \n",
    "    + theme(axis_text_x=element_text(angle=45, hjust=1))\n",
    "    + scale_x_datetime(date_breaks=\"1 week\")\n",
    "    + expand_limits(y=0)\n",
    "    + labs(\n",
    "        y = \"Total service hours\",\n",
    "        x = \"Service date\",\n",
    "        #title=\"Service hours per day\"\n",
    "    )\n",
    ")\n",
    "\n",
    "p.save(out_dir / \"2_service_hours.png\", width=4, height=4, dpi=300)\n",
    "#p.save(out_dir / \"2_service_hours.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edasmalchi/.venv/reports/lib/python3.9/site-packages/plotnine/ggplot.py:719: PlotnineWarning: Saving 4 x 4 in image.\n",
      "/Users/edasmalchi/.venv/reports/lib/python3.9/site-packages/plotnine/ggplot.py:722: PlotnineWarning: Filename: outputs/2022/05/4/data/3_id_changes.png\n"
     ]
    }
   ],
   "source": [
    "## Percent Change Plot\n",
    "\n",
    "p = (\n",
    "    pd.concat(\n",
    "        [\n",
    "            tbl_stops_changed >> mutate(kind=\"Stops\"),\n",
    "            tbl_routes_changed >> mutate(kind=\"Routes\"),\n",
    "        ]\n",
    "    )\n",
    "    >> mutate(status = fct_rev(_.status))    \n",
    "    >> ggplot(aes(\"kind\", \"percent\", fill=\"status\"))\n",
    "    + geom_col()\n",
    "    + scale_fill_manual(limits = CHANGE_PLOT_BREAKS, values = CHANGE_PLOT_VALUES)\n",
    "    + labs(\n",
    "        x=\"GTFS schedule table\",\n",
    "        y=\"Percentage of IDs\",\n",
    "        #title=f\"IDs Changed Between {START_MONTH_DAY} and {END_MONTH_DAY}\",\n",
    "    )\n",
    "    + scale_y_continuous(labels=percent_format, breaks=np.arange(0, 1.2, 0.2))\n",
    "    + theme_minimal(base_size=16)\n",
    "    + theme(legend_position=\"bottom\", legend_margin=30)\n",
    ")\n",
    "\n",
    "p.save(out_dir / \"3_id_changes.png\", width=4, height=4, dpi=300)\n",
    "# p.draw();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
